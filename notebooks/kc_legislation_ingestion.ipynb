{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingesting Turkish Legislation Data from MulteciHukuku.net\n",
    "This notebook extracts national legislation documents from the Turkish-language web resource, MulteciHukuku.net, for later use in a RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Extracting Legislation Document URLs\n",
    "In order to scrape national legislation document contents, we first need to assemble a list of available documents. We'll do so by extracting the title and href of all relevant documents listed on the \"National Legislation\" (*Ulusal Mevzuat*) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def stringify_children(node):\n",
    "    \"\"\"\n",
    "    Convert the children of a node into a single string, maintaining order.\n",
    "    \"\"\"\n",
    "    parts = ([node] if node.string else []) + list(node.children)\n",
    "    return ''.join(str(x) for x in parts)\n",
    "\n",
    "\n",
    "def extract_urls(url):\n",
    "    \"\"\"\n",
    "    Extract the individual document urls from a given URL.\n",
    "    \"\"\"\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    # Find the element that contains the legislation cards\n",
    "    data = soup.find(\"div\", id=\"nav-national-legislation\")\n",
    "    cards = data.select('div[class*=\"ps-document-card-type-legislation\"]')\n",
    "    urls = []\n",
    "\n",
    "    for index, card in enumerate(cards):\n",
    "        # Extract the legislation link from the card's anchor element\n",
    "        card_title = card.find(\"h2\").get_text(strip=True)\n",
    "        card_link = card.find(\"a\")['href']\n",
    "\n",
    "        # Append the URL \n",
    "        urls.append(\n",
    "            {\n",
    "                \"name\": card_title,\n",
    "                \"link\": card_link\n",
    "            })\n",
    "\n",
    "    return urls\n",
    "\n",
    "\n",
    "def save_urls_to_file(filename, url):\n",
    "    \"\"\"\n",
    "    Extract and save URLs to a JSON file.\n",
    "    \"\"\"\n",
    "    extracted_urls = extract_urls(url)\n",
    "    \n",
    "    # Ensure the 'faq_data' subdirectory exists, or create it if not\n",
    "    os.makedirs(\"../data/processed/legislation_data\", exist_ok=True)\n",
    "\n",
    "    # Save the aggregated FAQ data to a specified JSON file\n",
    "    with open(os.path.join(\"../data/processed/legislation_data/\", f\"{filename}.json\"), \"w\") as outfile:\n",
    "        json.dump(extracted_urls, outfile, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the save_urls_to_file function, saving the results as a JSON file.\n",
    "save_urls_to_file(filename=\"National_Legislation_TR\", url = \"https://multecihukuku.net/mevzuat/?document-type=ulusal-mevzuat\")\n",
    "\n",
    "# To make things easier in the next steps, we'll also save just a list of the extracted URLs by calling the extract_urls function and mapping through it to extract the links.\n",
    "legislation_urls = extract_urls(\"https://multecihukuku.net/mevzuat/?document-type=ulusal-mevzuat\")\n",
    "urls = [entry['link'] for entry in legislation_urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://multecihukuku.net/ulusal-mevzuat/acil-saglik-hizmetleri-yonetmeligi/\n"
     ]
    }
   ],
   "source": [
    "# Check the first entry in the urls list to make sure we extracted the urls properly\n",
    "print(urls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract Legislation Document Contents\n",
    "Next, we'll need to define a function to loop through the provided list of urls, extracting the contents of each legislation document. The structure of these pages is variable, but every document includes at least one of the following:\n",
    "- Block text;\n",
    "- Text split into one or more 'accordion' sections\n",
    "\n",
    "Finally, we'll save the resulting documents in a JSON file for later use in a RAG pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_legislation_data(url):\n",
    "    \"\"\"\n",
    "    Extract legislation data from a given URL.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    # Extract the page title up to the specified delimiter\n",
    "    page_title = soup.title.string.split(\" - \")[0]\n",
    "\n",
    "    # Initialize a list to hold all content\n",
    "    all_content = []\n",
    "\n",
    "    # Find the element containing the legislation block text and add it to all_content\n",
    "    block_text = soup.find(\"section\", class_=\"paragraph-text\")\n",
    "    if block_text is not None:\n",
    "        block_text_content = stringify_children(block_text)\n",
    "        all_content.append(block_text_content)\n",
    "\n",
    "    # Find accordion elements and add their content to all_content\n",
    "    accordion = soup.find(\"div\", class_=\"accordion\")\n",
    "    if accordion is not None:\n",
    "        cards = accordion.find_all(\"div\", class_=\"accordion-item\")\n",
    "        for card in cards:\n",
    "            title_element = card.find(\"button\")\n",
    "            title = title_element.get_text(strip=True)\n",
    "\n",
    "            content_element = card.find(\"div\", class_=\"accordion-body\")\n",
    "            content_html = stringify_children(content_element)\n",
    "\n",
    "            # You can format each section as you prefer, here we use a dict for each accordion section\n",
    "            all_content.append({\"section\": title, \"content\": content_html})\n",
    "\n",
    "    # Combine all content and append the URL to the legislation_data dict\n",
    "    legislation_data = {\n",
    "        \"title\": page_title,\n",
    "        \"content\": all_content,\n",
    "        \"url\": url\n",
    "    }\n",
    "    \n",
    "    return legislation_data\n",
    "\n",
    "\n",
    "\n",
    "def extract_from_urls(urls):\n",
    "    \"\"\"\n",
    "    Extract data from a list of urls.\n",
    "    \"\"\"\n",
    "    legislation_data = []\n",
    "\n",
    "    for url in (urls):\n",
    "        legislation_data.append(extract_legislation_data(url))\n",
    "    return legislation_data\n",
    "\n",
    "def save_data_to_file(filename, urls):\n",
    "    \"\"\"\n",
    "    Extract and save legislation content from a list of URLs to a JSON file.\n",
    "    \"\"\"\n",
    "\n",
    "    data = extract_from_urls(urls)\n",
    "    \n",
    "    # Ensure the 'faq_data' subdirectory exists, or create it if not\n",
    "    os.makedirs(\"../data/processed/legislation_data\", exist_ok=True)\n",
    "\n",
    "    # Save the aggregated FAQ data to a specified JSON file\n",
    "    with open(os.path.join(\"../data/processed/legislation_data/\", f\"{filename}.json\"), \"w\") as outfile:\n",
    "        json.dump(data, outfile, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data_to_file(\"National_Legislation_Content_TR\", urls = urls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
