{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Basic RAG QA with Langchain\n",
    "This notebook demonstrates how to implement a basic Retrieval-Augmented Generation (RAG) chain using our FAQ data. The overall approach is as follows:\n",
    "1. Load FAQ Data & Structure Metadata\n",
    "2. Create Embeddings for the FAQ Data using Amazon's Titan Embeddings model, and save these embeddings locally using Chroma.\n",
    "3. Create a Question Answering (QA) chain which retrieves context based on the embeddings saved in Chroma, serving these as context to the Amazon Titan Express LLM to answer the provided user prompt.\n",
    "4. Format the response so that source materials can be cited.\n",
    "\n",
    "This implementation mostly follows these Langchain tutorials:\n",
    "- https://python.langchain.com/docs/modules/data_connection/document_loaders/json#using-jsonloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metadata extraction function so we can filter on sections and return deep links as sources\n",
    "def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "    metadata[\"section\"] = record.get(\"section\")\n",
    "    metadata[\"source\"] = record.get(\"deep_link\")\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import JSON FAQ File using JSONLoader\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from pprint import pprint\n",
    "\n",
    "file_path='../data/processed/faq_data/EN_NSYR.json'\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path=file_path,\n",
    "    jq_schema=\".[]\",\n",
    "    content_key=\"answer\",\n",
    "    metadata_func=metadata_func\n",
    ")\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "embeddings = BedrockEmbeddings(\n",
    "    model_id=\"amazon.titan-embed-text-v1\", region_name=\"us-east-1\"\n",
    ")\n",
    "persistDirectory = './data/processed/faq_data/vectordata/EN_NSYR'\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=data, embedding=embeddings, persist_directory=persistDirectory)\n",
    "vectorstore.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cells below to re-use already-generated vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "embeddings = BedrockEmbeddings(\n",
    "    model_id=\"amazon.titan-embed-text-v1\", region_name=\"us-east-1\"\n",
    ")\n",
    "\n",
    "persistDirectory = './data/processed/faq_data/vectordata/EN_NSYR'\n",
    "# Retrieve and generate answers using relevant FAQs\n",
    "vectordb = Chroma(persist_directory=persistDirectory,\n",
    "                  embedding_function=embeddings)\n",
    "\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import CreateInferenceModifier # Import the function from utils.py\n",
    "from langchain import hub\n",
    "from langchain_community.llms import Bedrock\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Drawing from your knowledge base, answer the question below.\n",
    "If you don't know the answer from the provided context, tell me that your training materials don't include this information.\n",
    "Keep the answer as concise and relevant to the question as possible.\n",
    "\n",
    "Knowledge Base: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "\n",
    "# Define the universal set of modifier parameters\n",
    "modifiers = {\"max_tokens\": 4000,\n",
    "             \"temperature\": 0.2,\n",
    "             \"top_k\": 250,\n",
    "             \"top_p\": 1,\n",
    "             \"stop_sequences\": [\"\\n\\nHuman\"],\n",
    "             }\n",
    "\n",
    "\n",
    "llm = Bedrock(model_id=\"anthropic.claude-v2:1\",\n",
    "              model_kwargs=CreateInferenceModifier(\"claude\", params=modifiers))\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Answer: \n",
       "\n",
       "  Based on the knowledge base provided, to study at a university in Turkey you need:\n",
       "\n",
       "1) To graduate from high school\n",
       "2) Meet other conditions for university entrance, such as taking the Foreigner Student Exam (Yabancı Öğrenci Sınavı – YÖS) and obtaining a score that meets the requirements of the university you want to attend\n",
       "3) If you graduated from a school outside of Turkey or cannot demonstrate previous school attendance, you need to register at Open Education High School (Açık Öğretim Lisesi)\n",
       "4) Pass the Examination for Foreign Students (YÖS) organized by universities\n",
       "5) Potentially demonstrate language proficiency depending on university/department requirements\n",
       "6) Consult university websites/foreign student departments for detailed requirements and announcements\n",
       "\n",
       "The key prerequisites are graduating high school and passing exams like YÖS to demonstrate academic readiness for university-level studies. The knowledge base also directs readers to university resources for complete and up-to-date information on enrollment requirements. \n",
       "\n",
       " ## Sources: \n",
       "\n",
       " https://multecihaklari.info/services/education/?section=questions&question=9; https://multecihaklari.info/services/rights-and-procedures-for-unaccompanied-minors/?section=questions&question=16; https://multecihaklari.info/services/rights-and-procedures-for-unaccompanied-minors-2/?section=questions&question=16"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to extract unique URLs used in the retrieved source materials.\n",
    "from IPython.display import Markdown, display\n",
    "def extract_unique_urls(response):\n",
    "    unique_urls = set()  # Use a set to store unique URLs\n",
    "    \n",
    "    # Iterate through each document in the 'context'\n",
    "    for document in response['context']:\n",
    "        source_url = document.metadata['source']  # Extract the 'source' URL\n",
    "        unique_urls.add(source_url)  # Add the URL to the set\n",
    "    \n",
    "    # Convert the set of unique URLs to a string\n",
    "    urls_string = '; '.join(unique_urls)\n",
    "    \n",
    "    return urls_string\n",
    "\n",
    "# Invoke the chain and print the response and sources.\n",
    "response = rag_chain_with_source.invoke(\"What do I need to study at a university in Turkey?\")\n",
    "answer = response[\"answer\"]\n",
    "prettyResponse = f\"## Answer: \\n\\n {answer} \\n\\n ## Sources: \\n\\n {extract_unique_urls(response)}\"\n",
    "Markdown(prettyResponse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
