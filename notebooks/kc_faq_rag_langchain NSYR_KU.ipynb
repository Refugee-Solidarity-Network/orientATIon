{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Basic RAG QA with Langchain\n",
    "This notebook demonstrates how to implement a basic Retrieval-Augmented Generation (RAG) chain using our FAQ data. The overall approach is as follows:\n",
    "1. Load FAQ Data & Structure Metadata\n",
    "2. Create Embeddings for the FAQ Data using Amazon's Titan Embeddings model, and save these embeddings locally using Chroma.\n",
    "3. Create a Question Answering (QA) chain which retrieves context based on the embeddings saved in Chroma, serving these as context to the Amazon Titan Express LLM to answer the provided user prompt.\n",
    "4. Format the response so that source materials can be cited.\n",
    "\n",
    "This implementation mostly follows these Langchain tutorials:\n",
    "- https://python.langchain.com/docs/modules/data_connection/document_loaders/json#using-jsonloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metadata extraction function so we can filter on sections and return deep links as sources\n",
    "def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "    metadata[\"section\"] = record.get(\"section\")\n",
    "    metadata[\"source\"] = record.get(\"deep_link\")\n",
    "    \n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import JSON FAQ File using JSONLoader\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from pprint import pprint\n",
    "\n",
    "file_path='../data/processed/faq_data/KU_NSYR.json'\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path=file_path,\n",
    "    jq_schema=\".[]\",\n",
    "    content_key=\"answer\",\n",
    "    metadata_func=metadata_func\n",
    ")\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "embeddings = BedrockEmbeddings(\n",
    "    model_id=\"amazon.titan-embed-text-v1\", region_name=\"us-east-1\"\n",
    ")\n",
    "persistDirectory = './data/processed/faq_data/vectordata/KU_NSYR'\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=data, embedding=embeddings, persist_directory=persistDirectory)\n",
    "vectorstore.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cells below to re-use already-generated vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "embeddings = BedrockEmbeddings(\n",
    "    model_id=\"amazon.titan-embed-text-v1\", region_name=\"us-east-1\"\n",
    ")\n",
    "\n",
    "persistDirectory = './data/processed/faq_data/vectordata/KU_NSYR'\n",
    "# Retrieve and generate answers using relevant FAQs\n",
    "vectordb = Chroma(persist_directory=persistDirectory,\n",
    "                  embedding_function=embeddings)\n",
    "\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import CreateInferenceModifier # Import the function from utils.py\n",
    "from langchain import hub\n",
    "from langchain_community.llms import Bedrock\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Drawing from your knowledge base, answer the question below.\n",
    "If you don't know the answer from the provided context, tell me that your training materials don't include this information.\n",
    "Keep the answer as concise and relevant to the question as possible.\n",
    "\n",
    "Knowledge Base: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "\n",
    "# Define the universal set of modifier parameters\n",
    "modifiers = {\"max_tokens\": 4000,\n",
    "             \"temperature\": 0.2,\n",
    "             \"top_k\": 250,\n",
    "             \"top_p\": 1,\n",
    "             \"stop_sequences\": [\"\\n\\nHuman\"],\n",
    "             }\n",
    "\n",
    "\n",
    "llm = Bedrock(model_id=\"anthropic.claude-v2:1\",\n",
    "              model_kwargs=CreateInferenceModifier(\"claude\", params=modifiers))\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | custom_rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_with_source = RunnableParallel(\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    ").assign(answer=rag_chain_from_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Answer: \n",
       "\n",
       "  بۆ داواکردنی پەنابەری لە تورکیا، ئەم هەنگاوانەت پێویستە:\n",
       "\n",
       "1. تۆمارکردن لەگەڵ بەڕێوەبەرایەتیی هەرێمی بۆ بەڕێوەبردنی کۆچ و تۆمارکردنی داخوازینامەی پاراستنی نێودەوڵەتی\n",
       "\n",
       "2. چاوپێکەوتنی تۆمارکردن لەگەڵ بەرپرسان و پاسخدانەوەی پرسیارەکانیان دەربارەی هۆکارەکانی جێهێشتن \n",
       "\n",
       "3. چاوپێکەوتنی کەسی بۆ شرۆڤەکردنی هۆکارەکانی جێهێشتن بە تێروتەسەلی \n",
       "\n",
       "4. چاوەڕوان بە وەڵامی هەڵسەنگاندنی داخوازینامەکە \n",
       "\n",
       "5. ئەگەر وەڵام ئەرێنی بوو، وەرگرتنی بەڵگەنامەی ناسنامەی داخوازیکەری پاراستنی نێودەوڵەتی\n",
       "\n",
       "6. درێژکردنەوەی ماوەی بەڵگەنامەی ناسنامە بە سەردانکردنی بەڕێوەبەرایەتیی هەرێمی بۆ بەڕێوەبردنی کۆچ  \n",
       "\n",
       "ئەمانە گرنگترین هەنگاوەکانن بۆ داواکردن و وەرگرتنی پەنابەری لە تورکیا. \n",
       "\n",
       " ## Sources: \n",
       "\n",
       " https://multecihaklari.info/ku/services/labor-market-4/?section=questions&question=0; https://multecihaklari.info/ku/services/registration-and-status-2/?section=questions&question=6; https://multecihaklari.info/ku/services/detention-4/?section=questions&question=0; https://multecihaklari.info/ku/services/legal-assistance-4/?section=questions&question=2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a function to extract unique URLs used in the retrieved source materials.\n",
    "from IPython.display import Markdown, display\n",
    "def extract_unique_urls(response):\n",
    "    unique_urls = set()  # Use a set to store unique URLs\n",
    "    \n",
    "    # Iterate through each document in the 'context'\n",
    "    for document in response['context']:\n",
    "        source_url = document.metadata['source']  # Extract the 'source' URL\n",
    "        unique_urls.add(source_url)  # Add the URL to the set\n",
    "    \n",
    "    # Convert the set of unique URLs to a string\n",
    "    urls_string = '; '.join(unique_urls)\n",
    "    \n",
    "    return urls_string\n",
    "\n",
    "\n",
    "# Invoke the chain and print the response and sources.\n",
    "response = rag_chain_with_source.invoke(\n",
    "    \"چی هەنگاوەکانم پێویستە بنێم بۆ داواکردنی پەنابەری لە تورکیا؟\")\n",
    "answer = response[\"answer\"]\n",
    "prettyResponse = f\"## Answer: \\n\\n {answer} \\n\\n ## Sources: \\n\\n {extract_unique_urls(response)}\"\n",
    "Markdown(prettyResponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_asylum_system(question):\n",
    "    from IPython.display import Markdown, display\n",
    "    # Invoke the LLM chain with the provided question\n",
    "    response = rag_chain_with_source.invoke(question)\n",
    "\n",
    "    # Extract the answer from the response\n",
    "    answer = response[\"answer\"]\n",
    "\n",
    "    # Format the response using Markdown\n",
    "    pretty_response = f\"## Answer: \\n\\n{answer}\\n\\n## Sources:\\n\\n{extract_unique_urls(response)}\"\n",
    "    return pretty_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Answer: \n",
       "\n",
       " بەپێی زانیارییەکانی ناو زانیارینامەکەم، ئەمانە مافە سەرەکییەکانت هەیە وەکو پەنابەرێک لە تورکیا:\n",
       "\n",
       "- مافی داواکردنی پەنابەری و مافی پاراستن لە دیپۆرتکردنەوە\n",
       "- مافی کارکردن و مافی خوێندن\n",
       "- مافی تەندروستی و چاودێریی تەندروستی\n",
       "- مافی دەستڕاگەیشتن بە شوێنی نیشتەجێبوون و خواردن و پارەی گیرفان\n",
       "- مافی یەکگرتنەوەی خێزان و مافی دامەزراندنی خێزان\n",
       "- مافی ئازادیی بیروڕا و ئایین و ڕادەربڕین\n",
       "\n",
       "ئەم مافانە بەشێوەیەکی گشتی لەلایەن یاساکانی تورکیاوە پارێزراون بۆ پەنابەران. بۆ زانیاری زیاتر سەبارەت بە مافەکانت، پێشنیار دەکەم پەیوەندی بە یەکێک لە ڕێکخراوە ماف-پارێزەکان بکەیت.\n",
       "\n",
       "## Sources:\n",
       "\n",
       "https://multecihaklari.info/ku/services/unaccompanied-minors-4/?section=questions&question=13; https://multecihaklari.info/ku/services/unaccompanied-minors-4/?section=questions&question=9; https://multecihaklari.info/ku/services/detention-4/?section=questions&question=0; https://multecihaklari.info/ku/services/legal-assistance-4/?section=questions&question=2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(query_asylum_system(\n",
    "    \"ئایا دەتوانیت لێم بڵێیت چی مافەکانم هەیە وەکو پەنابەرێک لە تورکیا؟\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Answer: \n",
       "\n",
       " <p><span style=\"font-weight: 400;\">لە کۆتایی زانیارییەکەدا هاتووە کە:</span></p>\n",
       "\n",
       "<p><span style=\"font-weight: 400;\">\"هەروەها دوای دەرچوونت بە بێ مۆڵەت لە دامەزراوەیەکدا، ناتوانیت چیدی بەردەوام بیت لە سوودمەندبوون لە مافەکانت وەک خوێندن، دەستڕاگەیشتن بە چاودێری تەندروستی، دەستڕاگەیشتن بە شوێنی نیشتەجێبوون، خواردن، پارەی گیرفان و هتد.\"</span></p>\n",
       "\n",
       "<p><span style=\"font-weight: 400;\">لێرەدا \"پارەی گیرفان\" ئاماژە بە پارەیەک دەکات کە دەدرێت بە منداڵانی بێ سەرپەرشت بۆ یارمەتیدانیان لە ژیان. ئەم پارانە دەدرێن لەلایەن دەزگا حکومییەکانەوە بۆ منداڵانی بێ سەرپەرشت.</span>\n",
       "\n",
       "<p><span style=\"font-weight: 400;\">بەپێی زانیارییەکانی پێشکەشکراو، تەنیا منداڵانی بێ سەرپەرشت دەتوانن ئەم پارانە وەربگرن. </span></p>\n",
       "\n",
       "## Sources:\n",
       "\n",
       "https://multecihaklari.info/ku/services/unaccompanied-minors-4/?section=questions&question=13; https://multecihaklari.info/ku/services/labor-market-4/?section=questions&question=0; https://multecihaklari.info/ku/services/education-4/?section=questions&question=13; https://multecihaklari.info/ku/services/legal-assistance-4/?section=questions&question=2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(query_asylum_system(\"پارەی گیرفان چییە؟ کێ دەتوانێت بەدەستی بهێنێت؟\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
